ğŸ¤– Distributed AutoML Platform
_____________________________________________________________________________________________________________________________________________________________________________________________________________
A powerful, modular, and distributed AutoML platform built on a microservices architecture.
This platform enables users to upload datasets, receive AI-driven model recommendations, perform automated hyperparameter optimization, and deploy models as production-ready REST APIs.
The system is designed with scalability, reproducibility, and MLOps best practices in mind.
_____________________________________________________________________________________________________________________________________________________________________________________________________________
ğŸ› ï¸ Technology Stack
Core Technologies
Backend: Python (Flask, PyTorch, Scikit-learn, Ray)
Frontend: Streamlit
Infrastructure & MLOps
Object Storage: MinIO (S3-Compatible)
Experiment Tracking: MLflow
Database: PostgreSQL (one isolated instance per service)
Message Broker: NATS
Cache / Task Queue: Redis
DevOps
Containerization: Docker, Docker Compose
_____________________________________________________________________________________________________________________________________________________________________________________________________________
Prerequisites

Docker & Docker Compose
8 GB RAM (minimum)
16 GB RAM (recommended)
Installation
Clone the repository:
git clone MicroLearn_OrchestrateurAutoML_par_microservices
cd distributed-automl-platform
Launch all services:
docker-compose up -d --build

Access the frontend:
http://localhost:8501
_____________________________________________________________________________________________________________________________________________________________________________________________________________

ğŸ”„ Automated Workflow
The platform provides a fully automated data and model flow â€” no manual ID copying between steps.

ğŸ“Š Data Preparation
Upload a CSV dataset.
A unique Dataset ID is generated and automatically propagated across services.

ğŸ¤– Model Selection
Select the target column.
The system recommends the most suitable models and launches a batch training job.

ğŸš€ Training Monitor
Track training jobs in real time.
Model artifacts are stored in MinIO and logged in MLflow.

ğŸ“ˆ Model Evaluation
Compare trained models through interactive charts and metrics.
Selecting a model prepares it for deployment.

ğŸ§ª Hyperparameter Optimization
Fine-tune the selected model using advanced optimization strategies.

ğŸ“¦ Model Deployment
Deploy the best model in one click as a TorchServe REST API.
_____________________________________________________________________________________________________________________________________________________________________________________________________________
ğŸ“‚ Project Structure
.
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ data_preparer/    # Python / Flask
â”‚   â”œâ”€â”€ model_selector/   # Python / Flask
â”‚   â”œâ”€â”€ trainer/          # Python / PyTorch / Ray
â”‚   â”œâ”€â”€ evaluator/        # Python / Plotly
â”‚   â”œâ”€â”€ hyperopt/         # Python / Optuna / Redis
â”‚   â”œâ”€â”€ deployer/         # Python / TorchServe
â”‚   â”œâ”€â”€ orchestrator/     # Node.js / TypeScript
â”‚   â””â”€â”€ frontend/         # Streamlit
â”œâ”€â”€ deployments/          # Local & production deployment artifacts
â””â”€â”€ docker-compose.yml    # Multi-service orchestration
_____________________________________________________________________________________________________________________________________________________________________________________________________________
ğŸ—ï¸ Architecture Overview!
[architecture microservices ](https://github.com/user-attachments/assets/158e03d1-baae-40a1-a669-2f25a7cee999)
